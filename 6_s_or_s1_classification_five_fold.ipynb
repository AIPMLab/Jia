{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Set the global default font to Times New Roman\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# 1. Read the dataset\n",
    "data = pd.read_csv(\"D:/Apple/survival analysis/survival analysis/APPLE/t1+t1Gd+t2+flair/t1+t2+t1Gd+flair_all/s_combined_modalities_965.csv\")\n",
    "data = data.drop(['index', 'gender', 'age_at_index', 'OS', \"OS.time\"], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']  # Extract the feature matrix by dropping the 'label' column.\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Use StratifiedKFold for 5-fold cross-validation, ensuring balanced class distribution in each fold.\n",
    "\n",
    "# Store results for each fold\n",
    "auc_scores = []  # Store AUC scores\n",
    "accuracies = []  # Store accuracy scores\n",
    "sensitivities = []  # Store sensitivity scores\n",
    "specificities = []  # Store specificity scores\n",
    "precisions = []  # Store precision scores\n",
    "f1_scores = []  # Store F1-Score values\n",
    "model_net_benefits = []  # Store net benefits for each fold\n",
    "total_conf_matrix = np.zeros((2, 2))  # Initialize a cumulative confusion matrix\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Initialize and train the Random Forest model\n",
    "    model = RandomForestClassifier(n_estimators=300, random_state=42)  # Create a Random Forest classifier with 300 trees and a fixed random state\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities and labels\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Predict probabilities for the positive class.\n",
    "    y_pred = model.predict(X_test)  # Predict class labels.\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)  # Compute the Area Under the ROC Curve (AUC).\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Compute the accuracy score.\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix.\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()  # Extract true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP).\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = f1_score(y_test, y_pred)  # Compute the F1-Score.\n",
    "\n",
    "    # Accumulate the confusion matrix\n",
    "    total_conf_matrix += conf_matrix\n",
    "    \n",
    "    # Save the results\n",
    "    auc_scores.append(auc_score)\n",
    "    accuracies.append(accuracy)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "    f1_scores.append(f1)  # Save the F1-Score.\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=36)\n",
    "    plt.ylabel('True Positive Rate', fontsize=36)\n",
    "    plt.title(f'ROC Curve (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.legend(loc=\"lower right\", prop={'size': 30})\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set(font_scale=4)\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 60})\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=36)\n",
    "    plt.xlabel('Predicted labels', fontsize=36)\n",
    "    plt.ylabel('True labels', fontsize=36)\n",
    "    plt.title(f'Confusion Matrix (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Decision Curve Analysis (DCA)\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)  # Generate 100 evenly spaced thresholds between 0.01 and 0.99.\n",
    "    \n",
    "    def calculate_net_benefit(thresholds, y_true, y_proba):\n",
    "        net_benefits = []\n",
    "        for threshold in thresholds:\n",
    "            w = threshold / (1 - threshold)  # Weight for false positives\n",
    "            predictions = y_proba >= threshold  # Predictions based on the current threshold\n",
    "            tp = np.sum((predictions == 1) & (y_true == 1))  # True positives\n",
    "            fp = np.sum((predictions == 1) & (y_true == 0))  # False positives\n",
    "            net_benefit = tp - (fp * w)  # Calculate net benefit\n",
    "            net_benefits.append(net_benefit / len(y_true))\n",
    "        return net_benefits\n",
    "    \n",
    "    fold_net_benefits = calculate_net_benefit(thresholds, y_test, y_pred_proba)  # Calculate net benefits for the current fold.\n",
    "    model_net_benefits.append(fold_net_benefits)  # Store the net benefits for the current fold.\n",
    "\n",
    "# Plot the average DCA curve\n",
    "avg_net_benefits = np.mean(model_net_benefits, axis=0)  # Compute the average net benefits across all folds.\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, avg_net_benefits, label='Random Forest (Average)', color='red')\n",
    "plt.xlabel('Probability Threshold', fontsize=16)\n",
    "plt.ylabel('Net Benefit', fontsize=16)\n",
    "plt.title('Decision Curve Analysis (Average)', fontsize=18)\n",
    "plt.legend(loc='lower left', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([-0.1, 1])\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot the average confusion matrix\n",
    "avg_conf_matrix = total_conf_matrix / kf.get_n_splits()  # Compute the average confusion matrix by dividing the cumulative matrix by the number of folds.\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(font_scale=4)\n",
    "heatmap = sns.heatmap(avg_conf_matrix, annot=True, fmt='.2f', cmap='Blues', annot_kws={\"size\": 60})\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=36)\n",
    "plt.xlabel('Predicted labels', fontsize=36)\n",
    "plt.ylabel('True labels', fontsize=36)\n",
    "plt.title('Average Confusion Matrix', fontsize=36, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Output average results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.8f} ± {np.std(auc_scores):.8f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.8f} ± {np.std(accuracies):.8f}\")\n",
    "print(f\"Mean Sensitivity: {np.mean(sensitivities):.8f} ± {np.std(sensitivities):.8f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificities):.8f} ± {np.std(specificities):.8f}\")\n",
    "print(f\"Mean Precision: {np.mean(precisions):.8f} ± {np.std(precisions):.8f}\")\n",
    "print(f\"Mean F1-Score: {np.mean(f1_scores):.8f} ± {np.std(f1_scores):.8f}\")  # Output the mean and standard deviation of F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, precision_score, f1_score\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Set the global default font to Times New Roman\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# 1. Read the dataset\n",
    "data = pd.read_csv(\"D:/Apple/survival analysis/survival analysis/APPLE/t1+t1Gd+t2+flair/t1+t2+t1Gd+flair_all/s_combined_modalities_965.csv\")\n",
    "data = data.drop(['index', 'gender', 'age_at_index', 'OS', \"OS.time\"], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold, including AUC, accuracy, sensitivity, specificity, precision, F1-Score, and confusion matrix. This part is the same as the previous RF model.\n",
    "auc_scores = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "model_net_benefits = []  # Store net benefits for each fold\n",
    "total_conf_matrix = np.zeros((2, 2))  # Initialize a cumulative confusion matrix\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Initialize and train the XGBoost model\n",
    "    clf_XGB = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    # Initialize an XGBoost classifier with the following parameters:\n",
    "    # - `use_label_encoder=False`: Disables the use of the label encoder, which is deprecated in newer versions of XGBoost.\n",
    "    # - `eval_metric='logloss'`: Specifies the evaluation metric as log loss, which is commonly used for binary classification.\n",
    "    # - `random_state=42`: Sets a fixed random seed to ensure reproducibility of results.\n",
    "    clf_XGB.fit(X_train, y_train)\n",
    "    # Train the XGBoost classifier on the training data:\n",
    "    # - `X_train`: The feature matrix for the training set.\n",
    "    # - `y_train`: The target labels for the training set.\n",
    "    # The model learns patterns in the training data to predict the target labels.\n",
    "\n",
    "    # Predict probabilities and labels\n",
    "    y_pred_proba = clf_XGB.predict_proba(X_test)[:, 1]\n",
    "    # Predict the probabilities for the positive class (class 1) on the test set:\n",
    "    # - `X_test`: The feature matrix for the test set.\n",
    "    # - `predict_proba`: Returns the predicted probabilities for both classes (class 0 and class 1).\n",
    "    # - `[:, 1]`: Extracts the probabilities for the positive class (class 1).\n",
    "    y_pred = clf_XGB.predict(X_test)\n",
    "    # Predict the class labels for the test set:\n",
    "    # - `X_test`: The feature matrix for the test set.\n",
    "    # - `predict`: Returns the predicted class labels (0 or 1) based on the learned decision boundary.\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Accumulate the confusion matrix\n",
    "    total_conf_matrix += conf_matrix\n",
    "    \n",
    "    # Save the results\n",
    "    auc_scores.append(auc_score)\n",
    "    accuracies.append(accuracy)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=36)\n",
    "    plt.ylabel('True Positive Rate', fontsize=36)\n",
    "    plt.title(f'ROC Curve (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.legend(loc=\"lower right\", prop={'size': 30})\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set(font_scale=4)\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 60})\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=36)\n",
    "    plt.xlabel('Predicted labels', fontsize=36)\n",
    "    plt.ylabel('True labels', fontsize=36)\n",
    "    plt.title(f'Confusion Matrix (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Decision Curve Analysis (DCA). This is the same as the previous RF model.\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    \n",
    "    def calculate_net_benefit(thresholds, y_true, y_proba):\n",
    "        net_benefits = []\n",
    "        for threshold in thresholds:\n",
    "            w = threshold / (1 - threshold)  # Weight for false positives\n",
    "            predictions = y_proba >= threshold  # Predictions based on the current threshold\n",
    "            tp = np.sum((predictions == 1) & (y_true == 1))  # True positives\n",
    "            fp = np.sum((predictions == 1) & (y_true == 0))  # False positives\n",
    "            net_benefit = tp - (fp * w)  # Calculate net benefit\n",
    "            net_benefits.append(net_benefit / len(y_true))\n",
    "        return net_benefits\n",
    "    \n",
    "    fold_net_benefits = calculate_net_benefit(thresholds, y_test, y_pred_proba)\n",
    "    model_net_benefits.append(fold_net_benefits)\n",
    "\n",
    "# Plot the average DCA curve\n",
    "avg_net_benefits = np.mean(model_net_benefits, axis=0)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, avg_net_benefits, label='Random Forest (Average)', color='red')\n",
    "plt.xlabel('Probability Threshold', fontsize=16)\n",
    "plt.ylabel('Net Benefit', fontsize=16)\n",
    "plt.title('Decision Curve Analysis (Average)', fontsize=18)\n",
    "plt.legend(loc='lower left', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([-0.1, 1])\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot the average confusion matrix\n",
    "avg_conf_matrix = total_conf_matrix / kf.get_n_splits()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(font_scale=4)\n",
    "heatmap = sns.heatmap(avg_conf_matrix, annot=True, fmt='.2f', cmap='Blues', annot_kws={\"size\": 60})\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=36)\n",
    "plt.xlabel('Predicted labels', fontsize=36)\n",
    "plt.ylabel('True labels', fontsize=36)\n",
    "plt.title('Average Confusion Matrix', fontsize=36, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Output average results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.8f} ± {np.std(auc_scores):.8f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.8f} ± {np.std(accuracies):.8f}\")\n",
    "print(f\"Mean Sensitivity: {np.mean(sensitivities):.8f} ± {np.std(sensitivities):.8f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificities):.8f} ± {np.std(specificities):.8f}\")\n",
    "print(f\"Mean Precision: {np.mean(precisions):.8f} ± {np.std(precisions):.8f}\")\n",
    "print(f\"Mean F1-Score: {np.mean(f1_scores):.8f} ± {np.std(f1_scores):.8f}\")  # Output the mean and standard deviation of F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, precision_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Set the global default font to Times New Roman\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# 1. Read the dataset\n",
    "data = pd.read_csv(\"D:/Apple/survival analysis/survival analysis/APPLE/t1+t1Gd+t2+flair/t1+t2+t1Gd+flair_all/s_combined_modalities_965.csv\")\n",
    "data = data.drop(['index', 'gender', 'age_at_index', 'OS', \"OS.time\"], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold\n",
    "auc_scores = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "model_net_benefits = []  # Store net benefits for each fold\n",
    "total_conf_matrix = np.zeros((2, 2))  # Initialize a cumulative confusion matrix\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [1],  # Regularization strength\n",
    "    'solver': ['lbfgs'],  # Optimization algorithm\n",
    "    'penalty': ['l2'],  # Regularization type\n",
    "    'max_iter': [100]  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Data normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    model = LogisticRegression()  # Initialize a Logistic Regression model with default parameters.\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    # Perform hyperparameter tuning using GridSearchCV:\n",
    "    # - `model`: The Logistic Regression model to be tuned.\n",
    "    # - `param_grid`: The grid of hyperparameters to search (e.g., 'C', 'solver', 'penalty', 'max_iter').\n",
    "    # - `cv=5`: Use 5-fold cross-validation to evaluate each combination of hyperparameters.\n",
    "    # - `scoring='roc_auc'`: Use the Area Under the ROC Curve (AUC) as the evaluation metric.\n",
    "    # - `n_jobs=-1`: Use all available CPU cores for parallel processing.\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    # Fit the Logistic Regression model to the scaled training data.\n",
    "    # GridSearchCV will search through the hyperparameter grid and find the best combination based on the AUC score.\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict probabilities and labels\n",
    "    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Accumulate the confusion matrix\n",
    "    total_conf_matrix += conf_matrix\n",
    "    \n",
    "    # Save the results\n",
    "    auc_scores.append(auc_score)\n",
    "    accuracies.append(accuracy)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=36)\n",
    "    plt.ylabel('True Positive Rate', fontsize=36)\n",
    "    plt.title(f'ROC Curve (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.legend(loc=\"lower right\", prop={'size': 30})\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set(font_scale=4)\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 60})\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=36)\n",
    "    plt.xlabel('Predicted labels', fontsize=36)\n",
    "    plt.ylabel('True labels', fontsize=36)\n",
    "    plt.title(f'Confusion Matrix (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Decision Curve Analysis (DCA), same as the previous RF model\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    def calculate_net_benefit(thresholds, y_true, y_proba):\n",
    "        net_benefits = []\n",
    "        for threshold in thresholds:\n",
    "            w = threshold / (1 - threshold)  # Weight for false positives\n",
    "            predictions = y_proba >= threshold  # Predictions based on the current threshold\n",
    "            tp = np.sum((predictions == 1) & (y_true == 1))  # True positives\n",
    "            fp = np.sum((predictions == 1) & (y_true == 0))  # False positives\n",
    "            net_benefit = tp - (fp * w)  # Calculate net benefit\n",
    "            net_benefits.append(net_benefit / len(y_true))\n",
    "        return net_benefits\n",
    "    \n",
    "    fold_net_benefits = calculate_net_benefit(thresholds, y_test, y_pred_proba)\n",
    "    model_net_benefits.append(fold_net_benefits)\n",
    "\n",
    "# Plot the average DCA curve\n",
    "avg_net_benefits = np.mean(model_net_benefits, axis=0)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, avg_net_benefits, label='Logistic Regression (Average)', color='red')\n",
    "plt.xlabel('Probability Threshold', fontsize=16)\n",
    "plt.ylabel('Net Benefit', fontsize=16)\n",
    "plt.title('Decision Curve Analysis (Average)', fontsize=18)\n",
    "plt.legend(loc='lower left', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([-0.1, 1])\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot the average confusion matrix\n",
    "avg_conf_matrix = total_conf_matrix / kf.get_n_splits()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(font_scale=4)\n",
    "heatmap = sns.heatmap(avg_conf_matrix, annot=True, fmt='.2f', cmap='Blues', annot_kws={\"size\": 60})\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=36)\n",
    "plt.xlabel('Predicted labels', fontsize=36)\n",
    "plt.ylabel('True labels', fontsize=36)\n",
    "plt.title('Average Confusion Matrix', fontsize=36, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Output average results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.8f} ± {np.std(auc_scores):.8f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.8f} ± {np.std(accuracies):.8f}\")\n",
    "print(f\"Mean Sensitivity: {np.mean(sensitivities):.8f} ± {np.std(sensitivities):.8f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificities):.8f} ± {np.std(specificities):.8f}\")\n",
    "print(f\"Mean Precision: {np.mean(precisions):.8f} ± {np.std(precisions):.8f}\")\n",
    "print(f\"Mean F1-Score: {np.mean(f1_scores):.8f} ± {np.std(f1_scores):.8f}\")  # Output the mean and standard deviation of F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Set the global default font to Times New Roman\n",
    "rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# 1. Read the dataset\n",
    "data = pd.read_csv(\"D:/Apple/survival analysis/survival analysis/APPLE/t1+t1Gd+t2+flair/t1+t2+t1Gd+flair_all/s_combined_modalities_965.csv\")\n",
    "data = data.drop(['index', 'gender', 'age_at_index', 'OS', \"OS.time\"], axis=1)\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results for each fold\n",
    "auc_scores = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "precisions = []\n",
    "f1_scores = []\n",
    "model_net_benefits = []  # Store net benefits for each fold\n",
    "total_conf_matrix = np.zeros((2, 2))  # Initialize a cumulative confusion matrix\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {'C': [1], 'kernel': ['linear'], 'class_weight': [None]}\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Data normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Perform hyperparameter tuning using GridSearchCV\n",
    "    model = GridSearchCV(SVC(probability=True), param_grid, cv=5, scoring='roc_auc')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    best_model = model.best_estimator_\n",
    "    print(\"Best parameters found: \", model.best_params_)\n",
    "    \n",
    "    # Predict probabilities and labels\n",
    "    y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Accumulate the confusion matrix\n",
    "    total_conf_matrix += conf_matrix\n",
    "    \n",
    "    # Save the results\n",
    "    auc_scores.append(auc_score)\n",
    "    accuracies.append(accuracy)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=36)\n",
    "    plt.ylabel('True Positive Rate', fontsize=36)\n",
    "    plt.title(f'ROC Curve (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.legend(loc=\"lower right\", prop={'size': 30})\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set(font_scale=4)\n",
    "    heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 60})\n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=36)\n",
    "    plt.xlabel('Predicted labels', fontsize=36)\n",
    "    plt.ylabel('True labels', fontsize=36)\n",
    "    plt.title(f'Confusion Matrix (Fold {fold + 1})', fontsize=36, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "    # Decision Curve Analysis (DCA)\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    \n",
    "    def calculate_net_benefit(thresholds, y_true, y_proba):\n",
    "        net_benefits = []\n",
    "        for threshold in thresholds:\n",
    "            w = threshold / (1 - threshold)  # Weight for false positives\n",
    "            predictions = y_proba >= threshold  # Predictions based on the current threshold\n",
    "            tp = np.sum((predictions == 1) & (y_true == 1))  # True positives\n",
    "            fp = np.sum((predictions == 1) & (y_true == 0))  # False positives\n",
    "            net_benefit = tp - (fp * w)  # Calculate net benefit\n",
    "            net_benefits.append(net_benefit / len(y_true))\n",
    "        return net_benefits\n",
    "    \n",
    "    fold_net_benefits = calculate_net_benefit(thresholds, y_test, y_pred_proba)\n",
    "    model_net_benefits.append(fold_net_benefits)\n",
    "\n",
    "# Plot the average DCA curve\n",
    "avg_net_benefits = np.mean(model_net_benefits, axis=0)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, avg_net_benefits, label='SVM (Average)', color='red')\n",
    "plt.xlabel('Probability Threshold', fontsize=16)\n",
    "plt.ylabel('Net Benefit', fontsize=16)\n",
    "plt.title('Decision Curve Analysis (Average)', fontsize=18)\n",
    "plt.legend(loc='lower left', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([-0.1, 1])\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot the average confusion matrix\n",
    "avg_conf_matrix = total_conf_matrix / kf.get_n_splits()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(font_scale=4)\n",
    "heatmap = sns.heatmap(avg_conf_matrix, annot=True, fmt='.2f', cmap='Blues', annot_kws={\"size\": 60})\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=36)\n",
    "plt.xlabel('Predicted labels', fontsize=36)\n",
    "plt.ylabel('True labels', fontsize=36)\n",
    "plt.title('Average Confusion Matrix', fontsize=36, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Output average results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.8f} ± {np.std(auc_scores):.8f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.8f} ± {np.std(accuracies):.8f}\")\n",
    "print(f\"Mean Sensitivity: {np.mean(sensitivities):.8f} ± {np.std(sensitivities):.8f}\")\n",
    "print(f\"Mean Specificity: {np.mean(specificities):.8f} ± {np.std(specificities):.8f}\")\n",
    "print(f\"Mean Precision: {np.mean(precisions):.8f} ± {np.std(precisions):.8f}\")\n",
    "print(f\"Mean F1-Score: {np.mean(f1_scores):.8f} ± {np.std(f1_scores):.8f}\")  # Output the mean and standard deviation of F1-Score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-GPU-apple-vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
